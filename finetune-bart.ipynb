{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "essays_train = pd.DataFrame(pd.read_csv('/Users/aryan/Actual-Coding/CDAC/feedback-prize-english-language-learning/train.csv'))\n",
    "essays = essays_train['full_text']\n",
    "train_texts = essays[1:2].to_list()\n",
    "coh_train = essays_train['cohesion']\n",
    "train_labels = coh_train[1:2].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1779, 10, 936, 16, 10, 464, 47, 33, 7, 905, 24, 109, 5, 275, 15, 47, 117, 948, 99, 16, 2909, 24, 64, 464, 110, 1508, 4, 2128, 47, 240, 7, 3874, 62, 8, 356, 99, 16, 198, 47, 142, 1272, 32, 5, 275, 169, 7, 464, 99, 47, 236, 7, 464, 552, 86, 536, 4, 83, 50118, 50118, 41045, 16, 10, 464, 13, 47, 142, 24, 64, 146, 47, 192, 430, 8, 244, 47, 7, 1346, 141, 326, 1033, 885, 1638, 4, 50118, 50118, 10993, 9, 70, 24, 64, 146, 47, 192, 430, 172, 5, 643, 4, 286, 1246, 939, 2145, 14, 77, 939, 376, 7, 5, 315, 532, 939, 206, 14, 1085, 21, 164, 7, 464, 162, 142, 939, 206, 14, 1085, 21, 164, 7, 464, 162, 142, 960, 21, 430, 14, 127, 247, 8, 172, 939, 588, 661, 14, 1593, 142, 10, 936, 189, 464, 47, 53, 2128, 64, 45, 464, 5, 169, 24, 16, 6, 53, 939, 2145, 14, 939, 21, 269, 9152, 53, 939, 206, 14, 464, 10, 319, 142, 2128, 127, 1272, 146, 162, 206, 14, 89, 16, 55, 631, 14, 939, 393, 192, 11, 127, 301, 53, 939, 95, 240, 7, 192, 24, 31, 10, 430, 169, 8, 33976, 905, 1085, 1102, 8, 910, 12448, 5, 464, 14, 939, 236, 7, 146, 142, 9, 95, 10, 936, 4, 286, 1246, 939, 206, 14, 1085, 21, 164, 7, 464, 162, 8, 14, 939, 33976, 240, 7, 28, 9152, 5988, 1059, 939, 240, 7, 386, 1782, 960, 11, 10, 430, 1319, 142, 47, 64, 120, 7758, 23, 358, 65, 53, 47, 240, 7, 216, 99, 16, 164, 7, 1102, 71, 6, 50118, 50118, 11970, 189, 192, 47, 430, 53, 5, 129, 169, 14, 47, 216, 141, 7, 464, 16, 7, 109, 5, 275, 8, 218, 75, 905, 1085, 50, 45, 809, 7, 464, 1085, 59, 47, 4, 20, 169, 47, 236, 7, 464, 45, 65, 33, 14, 8, 64, 75, 109, 1085, 59, 24, 142, 16, 110, 2031, 8, 110, 1272, 8, 47, 64, 2845, 99, 7, 109, 19, 24, 4, 50118, 50118, 10815, 9, 70, 64, 244, 47, 7, 1346, 141, 383, 173, 4, 286, 4327, 127, 3795, 33, 10, 319, 9, 1272, 53, 79, 33, 3975, 77, 79, 16, 198, 82, 6, 127, 3795, 16, 13207, 9, 239, 8, 939, 437, 45, 13207, 9, 239, 939, 222, 45, 1346, 596, 127, 25104, 16, 13207, 9, 239, 8, 11, 45, 13207, 9, 239, 8, 358, 86, 939, 192, 127, 3795, 11, 10, 16847, 24, 146, 162, 7923, 142, 79, 16, 13207, 8, 16, 6269, 6, 53, 939, 192, 24, 31, 10, 430, 169, 8, 939, 101, 5, 239, 53, 67, 79, 33, 7, 1346, 14, 1368, 3540, 383, 173, 11, 97, 82, 142, 24, 64, 117, 28, 5, 276, 25, 47, 4, 286, 1246, 939, 206, 14, 127, 3795, 8, 162, 32, 430, 142, 52, 32, 8, 939, 33, 7, 1346, 14, 79, 473, 45, 101, 239, 8, 939, 240, 7, 1346, 14, 4, 7, 244, 951, 7, 1346, 141, 383, 173, 47, 240, 7, 386, 7, 192, 141, 383, 173, 11, 14, 5151, 301, 4, 50118, 50118, 250, 936, 16, 10, 464, 13, 47, 8, 64, 146, 47, 10, 430, 8, 244, 47, 7, 1346, 4, 7632, 34, 10, 430, 2979, 8, 10, 430, 21, 7, 1346, 172, 643, 4, 961, 64, 192, 5, 430, 2979, 8, 99, 97, 82, 206, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBartForSequenceClassification\n",
    "\n",
    "model = TFBartForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001 # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) # type: ignore\n",
    "model.fit(train_dataset.shuffle(1).batch(1), epochs=3, verbose = 1) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/Users/aryan/Actual-Coding/CDAC/feedback-prize-english-language-learning\") # type: ignore\n",
    "tokenizer.save_pretrained(\"/Users/aryan/Actual-Coding/CDAC/feedback-prize-english-language-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBartForSequenceClassification.from_pretrained(\"/Users/aryan/Actual-Coding/CDAC/feedback-prize-english-language-learning\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"/Users/aryan/Actual-Coding/CDAC/feedback-prize-english-language-learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
